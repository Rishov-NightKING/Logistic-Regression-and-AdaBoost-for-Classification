{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# importing necessary modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entropy calculate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://medium.com/@pytholabs/decision-trees-from-scratch-using-id3-python-coding-it-up-6b79e3458de4\n",
    "#  https://towardsdatascience.com/entropy-and-information-gain-in-decision-trees-c7db67a3a293\n",
    "def calc_entropy(dataframe, attribute):\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = np.bincount(dataframe[attribute])\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(dataframe[attribute])\n",
    "\n",
    "    entropy = 0\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            # use log from math and set base to 2\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "\n",
    "    return -entropy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## split dataset function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_dataset(dataframe, testsize=0.20):\n",
    "    X = dataframe.iloc[:, :-1]\n",
    "    y = dataframe.iloc[:, -1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize, random_state=1605084)\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_dataset1(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe\n",
    "    # removing unnecessary features\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    label_name = \"Churn\"\n",
    "\n",
    "    all_features = list(df.columns)\n",
    "    all_features.remove(label_name)\n",
    "    non_categorical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    categorical_features = list(set(all_features) - set(non_categorical_features))\n",
    "\n",
    "    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n",
    "    df.loc[:, label_name].replace(to_replace=['Yes', 'No'], value=[1, -1], inplace=True)\n",
    "\n",
    "    # all spaces are converted to np.nan\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df['TotalCharges'] = df['TotalCharges'].astype(float)\n",
    "\n",
    "    df.loc[:, 'SeniorCitizen'].replace([1, 0], [\"Yes\", \"No\"], inplace=True)\n",
    "\n",
    "    # removing data row with missing label\n",
    "    df.dropna(axis=0, how=\"any\", subset=[label_name], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # print(pd.isnull(df).sum().sum())\n",
    "\n",
    "    # feature = column name\n",
    "    for feature in df:\n",
    "        # fill missing values\n",
    "        if df[feature].isnull().sum() != 0:\n",
    "            # print(\"Column: \", feature, \" has missing values. count: \", df[feature].isnull().sum())\n",
    "            if feature in non_categorical_features:\n",
    "                # fill non-categorical missing values with median value -- https://www.geeksforgeeks.org/replacing-missing-values-using-pandas-in-python/\n",
    "                df[feature].fillna(df[feature].mean(), inplace=True)\n",
    "            else:\n",
    "                # fill categorical missing values with mode value -- https://www.tutorialspoint.com/python-pandas-filling-missing-column-values-with-mode\n",
    "                # mode returns a Series .. so we take the first element of the list\n",
    "                df[feature].fillna(df[feature].mode()[0], inplace=True)\n",
    "\n",
    "        if feature in non_categorical_features:\n",
    "            df[feature] = df[feature].astype(float)\n",
    "            # df[feature] = (df[feature]-df[feature].mean())/df[feature].std()\n",
    "\n",
    "            # df[feature] = (df[feature]-df[feature].min())/(df[feature].max() - df[feature].min())\n",
    "\n",
    "            x = df[feature].values.reshape(-1, 1)\n",
    "\n",
    "            standard_scaler = preprocessing.StandardScaler()\n",
    "            x_scaled = standard_scaler.fit_transform(x)\n",
    "\n",
    "            df[feature] = x_scaled\n",
    "\n",
    "    # https://stackabuse.com/one-hot-encoding-in-python-with-pandas-and-scikit-learn/\n",
    "\n",
    "    # one hot encoding implementation\n",
    "    df = pd.get_dummies(df, prefix=categorical_features, columns=categorical_features)\n",
    "\n",
    "    # move label column to the last\n",
    "    encoded_all_features = list(df.columns)\n",
    "    encoded_all_features.remove(label_name)\n",
    "    encoded_all_features.append(label_name)\n",
    "    df = df[encoded_all_features]\n",
    "\n",
    "    # splitting dataset to 80% training and 20% test\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df)\n",
    "    X_train[label_name] = y_train\n",
    "    X_test[label_name] = y_test\n",
    "    # fix indexing\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset 2 training and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_dataset2(train_filepath, test_filepath):\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                    'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "                    'income']\n",
    "    label_name = 'income'\n",
    "    non_categorical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    categorical_features = list(set(column_names) - set(non_categorical_features))\n",
    "    categorical_features.remove(label_name)\n",
    "\n",
    "    df1 = pd.read_csv(train_filepath, names=column_names, index_col=False, skipinitialspace=True)\n",
    "    df2 = pd.read_csv(test_filepath, names=column_names, index_col=False, skipinitialspace=True)\n",
    "\n",
    "    # # for training dataset\n",
    "    df1.loc[:, label_name].replace(to_replace=['<=50K', '>50K'], value=[-1, 1], inplace=True)\n",
    "    # for test dataset\n",
    "    df2.loc[:, label_name].replace(to_replace=['<=50K.', '>50K.'], value=[-1, 1], inplace=True)\n",
    "\n",
    "    train_len = df1.shape[0]\n",
    "\n",
    "    df = pd.concat([df1, df2])\n",
    "\n",
    "    df.replace(r'^[\\s?]+$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "    # removing data row with missing label\n",
    "    df.dropna(axis=0, how=\"any\", subset=[label_name], inplace=True)\n",
    "    df[label_name] = df[label_name].astype(np.int64)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # print(df.isnull().sum())\n",
    "\n",
    "    # feature = column name\n",
    "    for feature in df:\n",
    "        # fill missing values\n",
    "        if df[feature].isnull().sum() != 0:\n",
    "            # print(\"Column: \", feature, \" has missing values. count: \", df[feature].isnull().sum())\n",
    "            if feature in non_categorical_features:\n",
    "                # fill non-categorical missing values with median value -- https://www.geeksforgeeks.org/replacing-missing-values-using-pandas-in-python/\n",
    "                df[feature].fillna(df[feature].mean(), inplace=True)\n",
    "            else:\n",
    "                # fill categorical missing values with mode value -- https://www.tutorialspoint.com/python-pandas-filling-missing-column-values-with-mode\n",
    "                # mode returns a Series .. so we take the first element of the list\n",
    "                df[feature].fillna(df[feature].mode()[0], inplace=True)\n",
    "\n",
    "        if feature in non_categorical_features:\n",
    "            df[feature] = df[feature].astype(float)\n",
    "            x = df[feature].values.reshape(-1, 1)\n",
    "\n",
    "            standard_scaler = preprocessing.StandardScaler()\n",
    "            x_scaled = standard_scaler.fit_transform(x)\n",
    "\n",
    "            df[feature] = x_scaled\n",
    "    # https://stackabuse.com/one-hot-encoding-in-python-with-pandas-and-scikit-learn/\n",
    "    # one hot encoding implementation\n",
    "    df = pd.get_dummies(df, prefix=categorical_features, columns=categorical_features)\n",
    "    # move label column to the last\n",
    "    encoded_all_features = list(df.columns)\n",
    "    encoded_all_features.remove(label_name)\n",
    "    encoded_all_features.append(label_name)\n",
    "    df = df[encoded_all_features]\n",
    "\n",
    "    # splitting dataset\n",
    "    X_train = df.iloc[:train_len, :]\n",
    "    X_test = df.iloc[train_len:, :]\n",
    "    # fix indexing\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset3 full"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_dataset3(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop('Time', axis=1, inplace=True)\n",
    "    label_name = 'Class'\n",
    "    non_categorical_features = list(df.columns)\n",
    "    non_categorical_features.remove(label_name)\n",
    "\n",
    "    # removing data row with missing label\n",
    "    df.dropna(axis=0, how=\"any\", subset=[label_name], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.loc[:, label_name].replace(to_replace=[0], value=[-1], inplace=True)\n",
    "\n",
    "    # feature = column name\n",
    "    for feature in df:\n",
    "        # fill missing values\n",
    "        if df[feature].isnull().sum() != 0:\n",
    "            # print(\"Column: \", feature, \" has missing values. count: \", df[feature].isnull().sum())\n",
    "            if feature in non_categorical_features:\n",
    "                # fill non-categorical missing values with median value -- https://www.geeksforgeeks.org/replacing-missing-values-using-pandas-in-python/\n",
    "                df[feature].fillna(df[feature].mean(), inplace=True)\n",
    "            else:\n",
    "                # fill categorical missing values with mode value -- https://www.tutorialspoint.com/python-pandas-filling-missing-column-values-with-mode\n",
    "                # mode returns a Series .. so we take the first element of the list\n",
    "                df[feature].fillna(df[feature].mode()[0], inplace=True)\n",
    "\n",
    "        if feature in non_categorical_features:\n",
    "            df[feature] = df[feature].astype(float)\n",
    "            x = df[feature].values.reshape(-1, 1)\n",
    "\n",
    "            standard_scaler = preprocessing.StandardScaler()\n",
    "            x_scaled = standard_scaler.fit_transform(x)\n",
    "\n",
    "            df[feature] = x_scaled\n",
    "\n",
    "    # splitting dataset to 80% training and 20% test\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df)\n",
    "    X_train[label_name] = y_train\n",
    "    X_test[label_name] = y_test\n",
    "    # fix indexing\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dataset3 with all positive samples and variable number of negative samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_dataset3_sample(filepath, num_neg_samples=20000):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop('Time', axis=1, inplace=True)\n",
    "    label_name = 'Class'\n",
    "    non_categorical_features = list(df.columns)\n",
    "    non_categorical_features.remove(label_name)\n",
    "\n",
    "    # removing data row with missing label\n",
    "    df.dropna(axis=0, how=\"any\", subset=[label_name], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.loc[:, label_name].replace(to_replace=[0], value=[-1], inplace=True)\n",
    "\n",
    "    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n",
    "    positive_samples = df.loc[df[label_name] == 1]  # take all\n",
    "    negative_samples = (shuffle(df[df[label_name] == -1], random_state=84))[:num_neg_samples]  # take 20k\n",
    "\n",
    "    df = shuffle(pd.concat([positive_samples, negative_samples]), random_state=84).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv('dataset3/creditcard_sample.csv', index=False, header=True)\n",
    "\n",
    "    # splitting dataset to 80% training and 20% test\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df)\n",
    "    X_train[label_name] = y_train\n",
    "    X_test[label_name] = y_test\n",
    "    # fix indexing\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dataset4 given in online"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_dataset4(filepath):\n",
    "    df = pd.read_csv(filepath, sep=\";\", index_col=False, skipinitialspace=True)\n",
    "\n",
    "    column_names = [\"age\", \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\",\n",
    "                    \"day_of_week\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"poutcome\", \"emp.var.rate\",\n",
    "                    \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\", \"y\"]\n",
    "\n",
    "    label_name = 'y'\n",
    "\n",
    "    non_categorical_features = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx',\n",
    "                                \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"]\n",
    "    categorical_features = list(set(column_names) - set(non_categorical_features))\n",
    "    categorical_features.remove(label_name)\n",
    "\n",
    "    df.loc[:, label_name].replace(to_replace=['yes', 'no'], value=[1, -1], inplace=True)\n",
    "\n",
    "    # removing data row with missing label\n",
    "    df.dropna(axis=0, how=\"any\", subset=[label_name], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # feature = column name\n",
    "    for feature in df:\n",
    "        # fill missing values\n",
    "        if df[feature].isnull().sum() != 0:\n",
    "            print(\"Column: \", feature, \" has missing values. count: \", df[feature].isnull().sum())\n",
    "            if feature in non_categorical_features:\n",
    "                # fill non-categorical missing values with median value -- https://www.geeksforgeeks.org/replacing-missing-values-using-pandas-in-python/\n",
    "                df[feature].fillna(df[feature].mean(), inplace=True)\n",
    "            else:\n",
    "                # fill categorical missing values with mode value -- https://www.tutorialspoint.com/python-pandas-filling-missing-column-values-with-mode\n",
    "                # mode returns a Series .. so we take the first element of the list\n",
    "                df[feature].fillna(df[feature].mode()[0], inplace=True)\n",
    "\n",
    "        if feature in non_categorical_features:\n",
    "            df[feature] = df[feature].astype(float)\n",
    "            # df[feature] = (df[feature]-df[feature].mean())/df[feature].std()\n",
    "\n",
    "            # df[feature] = (df[feature]-df[feature].min())/(df[feature].max() - df[feature].min())\n",
    "\n",
    "            x = df[feature].values.reshape(-1, 1)\n",
    "\n",
    "            standard_scaler = preprocessing.StandardScaler()\n",
    "            x_scaled = standard_scaler.fit_transform(x)\n",
    "\n",
    "            df[feature] = x_scaled\n",
    "\n",
    "    #     # one hot encoding implementation\n",
    "    df = pd.get_dummies(df, prefix=categorical_features, columns=categorical_features)\n",
    "\n",
    "    # # move label column to the last\n",
    "    encoded_all_features = list(df.columns)\n",
    "    encoded_all_features.remove(label_name)\n",
    "    encoded_all_features.append(label_name)\n",
    "    df = df[encoded_all_features]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df, testsize=0.1)\n",
    "    X_train[label_name] = y_train\n",
    "    X_test[label_name] = y_test\n",
    "\n",
    "    # print(y_test.shape)\n",
    "    # fix indexing\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    return X_train, X_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2\n",
    "\n",
    "def train_loss(y, y_hat):\n",
    "    loss = np.mean((y - y_hat) * (y - y_hat))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, y_hat):\n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # y_hat --> hypothesis/predictions.\n",
    "    # w --> weights (parameter).\n",
    "\n",
    "    # Gradient of loss w.r.t weights.\n",
    "    dw = np.matmul(X.T, ((y - y_hat) * (1 - y_hat * y_hat))) / X.shape[0]\n",
    "    return dw\n",
    "\n",
    "\n",
    "def logistic_regression_train(data, epochs, lr, threshold=0.0):\n",
    "    # X --> Input.\n",
    "    # y --> target value.\n",
    "    # epochs --> Number of iterations.\n",
    "    # lr --> Learning rate.\n",
    "    # m-> number of training examples\n",
    "    # n-> number of features\n",
    "\n",
    "    X = data.iloc[:, :-1].values\n",
    "    X = np.insert(X, 0, [1] * len(X), axis=1)\n",
    "\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Initializing weights\n",
    "    w = np.zeros((n, 1))\n",
    "\n",
    "    # Reshaping y\n",
    "    y = y.reshape(m, 1)\n",
    "\n",
    "    losses = []\n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        # Calculating hypothesis/prediction.\n",
    "        y_hat = np.tanh(np.matmul(X, w))\n",
    "\n",
    "        # Getting the gradients of loss w.r.t parameters.\n",
    "        dw = gradient_descent(X, y, y_hat)\n",
    "\n",
    "        # Updating the parameters.\n",
    "        w += lr * dw\n",
    "\n",
    "        l = train_loss(y, y_hat)\n",
    "        if l < threshold:\n",
    "            # print(\"loss found less than 0.5.loss: \", l) # for weak learner threshold needs to be set\n",
    "            break\n",
    "        losses.append(l)\n",
    "\n",
    "    # plt.plot([i+1 for i  in range(epochs)], losses)\n",
    "    # plt.show()\n",
    "    # returning weights, predictions(hyp)\n",
    "    return w, y_hat\n",
    "\n",
    "\n",
    "def predict(data, weight_matrix):\n",
    "    # X --> Input.\n",
    "\n",
    "    X = data.iloc[:, :-1].values\n",
    "    X = np.insert(X, 0, [1] * len(X), axis=1)\n",
    "\n",
    "    # Calculating precisions/y_hat.\n",
    "\n",
    "    predictions = np.tanh(np.matmul(X, weight_matrix))\n",
    "    predictions_list = [-1 if x < 0 else 1 for x in predictions]\n",
    "    # predictions_list[predictions < 0] = -1\n",
    "\n",
    "    return np.array(predictions_list)\n",
    "\n",
    "\n",
    "def performance_measure(data, y_hat, show_all_measures=True):\n",
    "    y = data.iloc[:, -1]\n",
    "    acc = np.sum(y == y_hat) / len(y)\n",
    "    print(\"Accuracy: \", acc * 100, \"%\")\n",
    "\n",
    "    if show_all_measures:\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_hat).ravel()\n",
    "        TPR = tp / (tp + fn)  # recall, hit rate\n",
    "        TNR = tn / (tn + fp)\n",
    "        precision = tp / (tp + fp)\n",
    "        FDR = fp / (fp + tp)\n",
    "        f1 = (2 * precision * TPR) / (precision + TPR)\n",
    "\n",
    "        print(f\"True positive rate(sensitivity, recall, hit rate): {TPR * 100}%\")\n",
    "        print(f\"True negative rate (specificity): {TNR * 100}%\")\n",
    "        print(f\"Positive predictive value (precision): {precision * 100}%\")\n",
    "        print(f\"False discovery rate: {FDR * 100}%\")\n",
    "        print(f\"F1 score: {f1 * 100}%\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adaboost implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adaboost(examples, K):\n",
    "    # examples : set of N labled examples\n",
    "    # weak_learner: a learning algo\n",
    "    # K: number of hypothesis in the ensemble\n",
    "\n",
    "    # sample_size = N\n",
    "    sample_size = examples.shape[0]\n",
    "    epsilon = np.finfo(float).eps  # to avoid division by 0\n",
    "\n",
    "    # w: a vector of N example weights, initially 1/N\n",
    "    # h: a vector of K hypothesis\n",
    "    # z: a vector of K hypothesis weights\n",
    "    w = [float(1 / sample_size) for i in range(sample_size)]\n",
    "    h = []\n",
    "    z = []\n",
    "\n",
    "    real_label_values = examples.iloc[:, -1]\n",
    "    # print(\"hyihi\", real_label_values)\n",
    "    # print(\"shape1 : \", real_label_values[5])\n",
    "\n",
    "    for k in range(K):\n",
    "        data = examples.sample(n=sample_size, weights=w, replace=True, random_state=84).reset_index(drop=True)\n",
    "        weight, hypothesis = logistic_regression_train(data, epochs=100, lr=0.5, threshold=0.5)\n",
    "\n",
    "        h.append(weight)\n",
    "        error = 0\n",
    "\n",
    "        pred = predict(examples, weight_matrix=weight)\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            if pred[i] != real_label_values[i]:\n",
    "                error += w[i]\n",
    "\n",
    "        if error > 0.5:\n",
    "            # print(\"error > 0.5\")\n",
    "            continue\n",
    "\n",
    "        # print(\"k: \", k, \"    error: \", error)\n",
    "        for i in range(sample_size):\n",
    "            if pred[i] == real_label_values[i]:\n",
    "                w[i] = (w[i] * error) / (1 - error)\n",
    "\n",
    "        # normalize weights\n",
    "        w = [float(i) / sum(w) for i in w]\n",
    "\n",
    "        z.append(math.log((1 - error) / (error + epsilon), 2))\n",
    "\n",
    "    return h, z\n",
    "\n",
    "\n",
    "def adaboost_accuracy(dataset, hyp_vectors, z_values):\n",
    "    label_predict = [0] * dataset.shape[0]\n",
    "    for h, z in zip(hyp_vectors, z_values):\n",
    "        label_predict += predict(data=dataset, weight_matrix=h) * z\n",
    "\n",
    "    label_predict = [-1 if x < 0 else 1 for x in label_predict]\n",
    "    performance_measure(dataset, np.array(label_predict), show_all_measures=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# download datasets from the following links as they are too large to upload in github\n",
    "\n",
    "  <b>link for dataset1: https://www.kaggle.com/blastchar/telco-customer-churn</b>\n",
    "  <b>link for dataset2: https://archive.ics.uci.edu/ml/datasets/adult </b>\n",
    "  <b>link for dataset3: https://www.kaggle.com/mlg-ulb/creditcardfraud </b>\n",
    "  <b>link for dataset4: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing </b>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Specify datasets' file paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_1_file_path = \"dataset1/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "dataset_2_train_file_path = \"dataset2/adult.data\"\n",
    "dataset_2_test_file_path = \"dataset2/adult.test\"\n",
    "dataset_3_file_path = \"dataset3/creditcard.csv\"\n",
    "dataset_4_file_path = \"dataset4/bank-additional-full.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess data and get the train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get dataset1 train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_dataset1(dataset_1_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get dataset2 train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_dataset2(dataset_2_train_file_path, dataset_2_test_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get dataset3(full) train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_dataset3(dataset_3_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get dataset3(portion) train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_dataset3_sample(dataset_3_file_path, num_neg_samples=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### get dataset4 train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_dataset4(dataset_4_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Result:\")\n",
    "weight, predictions = logistic_regression_train(data=X_train, epochs=1000, lr=0.05)\n",
    "\n",
    "print(\"Training set:\")\n",
    "p = predict(data=X_train, weight_matrix=weight)\n",
    "performance_measure(data=X_train, y_hat=p)\n",
    "\n",
    "print(\"Test set:\")\n",
    "p = predict(data=X_test, weight_matrix=weight)\n",
    "performance_measure(data=X_test, y_hat=p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run adaboost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\\nAdaboost Result:\")\n",
    "K_values = [5, 10, 15, 20]\n",
    "for k in K_values:\n",
    "    print(\"\\nK :\", k)\n",
    "    print(\"Training set:\")\n",
    "    h, z = adaboost(X_train, k)\n",
    "    adaboost_accuracy(X_train, h, z)\n",
    "    print()\n",
    "\n",
    "    print(\"Test set: \")\n",
    "    h, z = adaboost(X_test, k)\n",
    "    adaboost_accuracy(X_test, h, z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}